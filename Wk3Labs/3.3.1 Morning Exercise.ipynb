{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 1 (No Python):**\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "On Monday we introduced the Poisson Distribution. The density with respect to the discrete measure is given by\n",
    "\n",
    "$$\\frac{dPois_{\\lambda}}{d\\mu_N}(k) = \\frac{\\lambda^k e^{-\\lambda}}{k!}$$\n",
    "\n",
    "Remember that the denisty $\\frac{dPois_{\\lambda}}{d\\mu_N}(k)$ is *defined* by the following equation for every possible event A:\n",
    "\n",
    "$$Pois_\\lambda(A) = \\int_{A}dPois_\\lambda(k) = \\int_{A}d\\mu_N(k) \\frac{dPois_{\\lambda}}{d\\mu_N}(k)$$\n",
    "\n",
    "The Poisson Distribution is used to model the number of occurances N within a fixed time frame, i.e. the number of hurricanes in one year or the number of car accidents during one month.\n",
    "\n",
    "We say that the random variable N is distributed with the Poisson Distribution if\n",
    "\n",
    "$$P(N \\in A) = P(N^{-1}(A)) = Pois_{\\lambda}(A) = \\int_{A}d\\mu_N(k) \\frac{dPois_{\\lambda}}{d\\mu_N}(k)$$\n",
    "\n",
    "The expected number of occurances is \n",
    "\n",
    "$$E[N] = \\int_{k \\geq 0} d\\mu_N(k) k \\frac{dPois_{\\lambda}}{d\\mu_N}(k) = \\int_{k \\geq 0} d\\mu_N(k) k \\frac{\\lambda^k e^{-\\lambda}}{k!} = \\lambda$$\n",
    "\n",
    "Thus the model parameter $\\lambda$ controls the expected number of occurances during a specific time frame.\n",
    "\n",
    "**Situation:**\n",
    "\n",
    "We are measuring the number of car accidents at a specific intersection over the course of 1 month. We find that the expected number of accidents on any given day is 0.5. State law requires (made up story) that the responsible county puts up a warning sign if the probability of more than 5 accidents during one day is larger than 0.01. \n",
    "\n",
    "Does the county have to put up a warning sign?\n",
    "\n",
    "Hint:\n",
    "* Use the complement rule namely $P(\\Omega \\setminus A) = P(\\Omega) - P(A) = 1 - P(A)$.\n",
    "* Use the fact that you can split any discrete set $A = \\{0,1,2,3,...\\}$ into disjoint unions $A = \\{0\\} \\uplus \\{1\\} \\uplus \\{2\\} \\uplus ...$.\n",
    "* Use the basic rule $P(A_1 \\uplus A_2) = P(A_1) + P(A_2)$ for any two disjoint sets $A_1, A_2$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " 0.00142%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "P0 = .5**0*np.exp(-.5)/1\n",
    "P1 = .5**1*np.exp(-.5)/1\n",
    "P2 = .5**2*np.exp(-.5)/2\n",
    "P3 = .5**3*np.exp(-.5)/6\n",
    "P4 = .5**4*np.exp(-.5)/24\n",
    "P5 = .5**5*np.exp(-.5)/120\n",
    "P6plus = 1 - P1 - P2 - P3 - P4 - P0 - P5\n",
    "print ('{0: .5%}'.format(P6plus))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Excersie 2 (Python):**\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "The normal distribution with respect to the Lebuege Measure is given by the following density\n",
    "\n",
    "$$\\frac{dNorm_{(\\lambda, \\mu)}}{d\\lambda_L}(x) = \\sqrt{\\frac{\\lambda}{2 \\pi}} e^{\\frac{-\\lambda (x-\\mu)^2}{2}}$$\n",
    "\n",
    "A continous random variable $X$ is said to be normally distributed if\n",
    "\n",
    "$$P(X \\in A) = Norm_{(\\lambda, \\mu)}(A) = \\int_{A}d\\lambda_L(x) \\sqrt{\\frac{\\lambda}{2 \\pi}} e^{\\frac{-\\lambda (x-\\mu)^2}{2}}$$\n",
    "\n",
    "The model parameter $\\mu$ controls expectation value by $E[X] = \\mu$ whereas the model parameter $\\lambda$ controls the variance (or squared standard error) $\\sigma^2 = Var[X] = \\frac{1}{\\lambda}$.\n",
    "\n",
    "**Situation:**\n",
    "\n",
    "You are an archeologist and you estimate that an old vase you found was produced bewtween 1510 and 1582. What is the probability that the vase does not originate from that time period assuming that the year X of production is normally distributed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3173105078629139"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import integrate\n",
    "\n",
    "# mid point btw years = 1546 is equal to expectation value = mu\n",
    "# error of +- 36 years (1546 - 1510)\n",
    "# error squared = 36**2 = 1296 = sigma squared\n",
    "# lambda = 1/1296\n",
    "\n",
    "norm = lambda l,m : lambda x: np.sqrt(l/(2.*np.pi)) * np.exp(-l/2.*(x-m)**2.)\n",
    "norm(1./1296.,1546.)(1510)\n",
    "\n",
    "P = 1 - integrate.quad(norm(1/1296., 1546), 1510,1582)[0] # take[0] because integrate.quad returns a tuple where the \n",
    "P"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Exercise 3 (Python):**\n",
    "\n",
    "**Introduction:**\n",
    "\n",
    "Let's look at mappings from one set to another in terms of the information that we have about that mapping. An example where we have all information available about a function would be the function $f :\\mathbb{R} \\to \\mathbb{R} \\,:f(x) = x^2$. For every value of $x \\in \\mathbb{R}$ we can calcualte the function value $f(x) = x^2$ for example $f(2) = 2^2 = 4$.\n",
    "\n",
    "Let's look at any random variable X. The random variable X is just a mapping that maps the event space $\\Omega$ onto another space for example $\\mathbb{R}$ (as in Exercise 2) or $\\mathbb{N}$ (as in Exercise 1). We just happned to have lost information about the mapping X since we are no longer able to map one particular point $\\omega_0 \\in \\Omega$ onto it's value $X(\\omega_0)$. In the setting of probability theory **all information** that we do have about a random variable $X$ is contained in the distribution $P_X$. \n",
    "\n",
    "Since all information about the random variable $X$ is contained within its distribution $P_X$ we always loose information about X if we use a different probability measure $Q$ to describe $X$ instead of $P_X$.\n",
    "\n",
    "The loss of information if we use $Q$ insead of $P_X$ is given by the **Kullback-Leibler Divergence** and is given by\n",
    "\n",
    "$$KL(P_X || Q) = - \\int_{\\Omega} dP_X log(\\frac{dQ}{dP_X})$$\n",
    "\n",
    "where the integral is taken over the entire event space $\\Omega$. Since the Kullback-Leibler Divergence reaches it's maximum if $Q = P_X$ we can use the Kullback-Leibler Divergence to make model selections namely by selecting that model which contains the most amount of information. This is called the **Maximum Entropy Principal** which you might know from physics.\n",
    "\n",
    "As a reminder:\n",
    "The binomial distribution is defined by\n",
    "\n",
    "$$\\frac{dBinom_{(p,N)}}{d\\mu_N} (k) = \\binom{N}{k}p^k(1-p)^{N-k}$$\n",
    "\n",
    "**Situation:**\n",
    "\n",
    "Let's look at the coin flipping experiment where we flip a coin N = 10 times and denote by $X$ the number of heads. Since we don't know better at this point we assume that $X$ is distributed with $Binom_{(p=0.5, N=10)}$ giving equal chances to heads and tails. \n",
    "\n",
    "What is the loss of information if the true distribution of X is actually $Binom_{(p=0.2, N=10)}$ ?\n",
    "\n",
    "Hint:\n",
    "* Use $\\frac{dBinom_{(p,N)}}{d\\mu_N} (k) = \\binom{N}{k}p^k(1-p)^{N-k}$ to express the integral as an integral over the discrete measure $\\mu_N$.\n",
    "* Use the relation $\\frac{dP}{dQ} = \\frac{dP}{d\\mu_N}\\frac{d\\mu_N}{dQ} = \\frac{dP}{d\\mu_N}(\\frac{dQ}{d\\mu_N})^{-1}$.\n",
    "* Use python to calculate the sum."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-1.9274475702175757"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from scipy.special import binom\n",
    "KL = lambda k: binom(10,k)*(.2**k)*(.8**(10-k))*np.log(((.5/.2)**k)*((.5/.8)**(10-k)))\n",
    "sum([KL(k) for k in range(11)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
